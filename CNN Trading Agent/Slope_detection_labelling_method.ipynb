{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Slope-detection labelling method.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Short-Term Stock Price-Trend Prediction Using Meta-Learning\n",
        "\n",
        "This notebook contains an adaptation of the *sliding time horizon* stocks labelling method proposed in `chang2021short`, referred to as \"Slope detection labelling\", originally designed for a meta-learning framework based on stock price dataset and other techincal indicators such as ATR, EMA20, MACD, ROC, ...\n",
        "\n",
        "The ultimate goal was to develop a similar labeling method based on LOB datasets features, in particular the *mid-prices*.\n",
        "\n",
        "For this purpose, I used the normalized midprices contained in FI-2020 dataset proposed in `ntakaris2018benchmark`.\n",
        "\n",
        "```\n",
        "@inproceedings{chang2021short,\n",
        "  title={Short-Term Stock Price-Trend Prediction Using Meta-Learning},\n",
        "  author={Chang, Shin-Hung and Hsu, Cheng-Wen and Li, Hsing-Ying and Zeng, Wei-Sheng and Ho, Jan-Ming},\n",
        "  booktitle={2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},\n",
        "  pages={2900--2905},\n",
        "  year={2021},\n",
        "  organization={IEEE}\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "@article{ntakaris2018benchmark,\n",
        "  title={Benchmark dataset for mid-price forecasting of limit order book data with machine learning methods},\n",
        "  author={Ntakaris, Adamantios and Magris, Martin and Kanniainen, Juho and Gabbouj, Moncef and Iosifidis, Alexandros},\n",
        "  journal={Journal of Forecasting},\n",
        "  volume={37},\n",
        "  number={8},\n",
        "  pages={852--866},\n",
        "  year={2018},\n",
        "  publisher={Wiley Online Library}\n",
        "}\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pNbktY7GnFYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This method was emplyed to define stock price trends known as: *rise+*, *rise* and *fall+*, *fall*.\n",
        "\n",
        "\n",
        "Consider the following metrics be computed on the same stock *n*, let:\n",
        "\n",
        "* $p_d$: closing price at day *d*\n",
        "* $F_d = \\sum_{i=1}^K p_{d+i}$: the average closing price in the next *K* days\n",
        "* $B_d = \\sum_{i=0}^{K+1} p_{d-i}$: the average closing price in the previous *K* days\n",
        "* $\\delta_d = F_d - B_d$: the *slope*\n",
        "* $\\mu_d$: the mean value of closing price\n",
        "* $\\sigma_d$: the standard deviation of the stock during the *2K* days, compute according to the following expression:\n",
        "\n",
        "$$ \\sigma_d = \\sqrt{\\frac{\\overset{K}{\\underset{i=-K+1}{\\sum}}(p_d+i-\\mu_d)^2}{2K}} $$\n",
        "\n",
        "$\\sigma_d$ is then applied as a threshold against the closing price, as follow:\n",
        "\n",
        "$$l(p_d) = \\begin{cases}\n",
        "    rise+,& \\text{if } p_d \\gt (\\mu_d + \\sigma_d) \\,\\text{ and }\\ \\delta_d \\gt 0\\\\\n",
        "    rise,& \\text{if } \\text{only } \\delta_d \\gt 0  \\text{ holds}\\\\\n",
        "    fall+,& \\text{if } p_d \\lt (\\mu_d - \\sigma_d) \\,\\text{ and }\\ \\delta_d \\lt 0\\\\ \n",
        "    fall,& \\text{if } \\text{only } \\delta_d \\lt 0  \\text{ holds}\\\\\n",
        "\\end{cases}$$ \n",
        "\n",
        "\\\\\n",
        "\n",
        "The $+$ sign must be read as \"peak\" or \"trough\", accordingly if it referred to *rise* or *fall*.\\\n",
        "*i.e.*: \"*rise+*\" mean that the stock has reach its maximum value and from that point it will decrease; viceversa, \"*fall+*\" means that from that point, the trend will raise"
      ],
      "metadata": {
        "id": "ZJRdZdqMyOYi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOqyrCGJx8aH",
        "outputId": "65a1b1ce-dc5f-4c10-c5ca-1d4613dbad1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Any, Dict, Optional, Tuple\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "eFhcFPsN73yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🦞 Data download and exploration"
      ],
      "metadata": {
        "id": "QZguZ8LUFHoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset info**:\n",
        "+ We used *no auction dataset* that is normalised by *decimal precision approach* \n",
        "+ The 'x_data' is a 2d-array that contains, for each row a snapshot of the orderbook in the following structure: 'best-ask price', 'best-ask volume', 'best-bid price', 'best-bid volume', '2-lev ask price', '2-levl ask volume', '2-lev bid price', '2-lev bid volume', ...\n",
        "+ The first 7 days are used as training data and the remaining 3 days are used for testing\n",
        "+ The training data are divided again with the 80/20 ratio in Training and Validation sets"
      ],
      "metadata": {
        "id": "vaXvLowM95ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datafolder = \"/content/drive/MyDrive/Colab Notebooks/AI4T/CNN Trading Agent/Data/\"\n",
        "\n",
        "# data_file paths\n",
        "train_file = \"train.txt\"\n",
        "val_file = \"val.txt\"\n",
        "test_file = \"test.txt\"\n",
        "\n",
        "# prepare the new paths\n",
        "new_train_file = \"new_train.txt\"\n",
        "new_val_file = \"new_val.txt\"\n",
        "new_test_file = \"new_test.txt\"\n",
        "\n",
        "\n",
        "if not all(\n",
        "    os.path.isfile(path) for path in [datafolder+train_file, datafolder+val_file, datafolder+test_file]\n",
        "):\n",
        "  # data paths\n",
        "  train_paths = datafolder+\"Train_Dst_NoAuction_DecPre_CF_7.txt\"\n",
        "  \n",
        "  test_paths = [\n",
        "                datafolder+\"Test_Dst_NoAuction_DecPre_CF_7.txt\",\n",
        "                datafolder+\"Test_Dst_NoAuction_DecPre_CF_8.txt\",\n",
        "                datafolder+\"Test_Dst_NoAuction_DecPre_CF_9.txt\",\n",
        "  ]\n",
        "\n",
        "  if not all(\n",
        "    os.path.isfile(path) for path in [train_paths] + test_paths\n",
        "             ):\n",
        "    !wget \"https://raw.githubusercontent.com/zcakhaa/DeepLOB-Deep-Convolutional-Neural-Networks-for-Limit-Order-Books/master/data/data.zip -P datafolder\n",
        "    !unzip -n datafolder+\"data.zip\" - d datafolder\n",
        "  \n",
        "  # load train + val data\n",
        "  train_val_data = np.loadtxt(train_paths, unpack=True)\n",
        "\n",
        "  # split into train and validation\n",
        "  train_slice = slice(0, int(0.8 * train_val_data.shape[0]))\n",
        "  val_slice = slice(int(0.8 * train_val_data.shape[0]), train_val_data.shape[0])\n",
        "\n",
        "  train_data = train_val_data[train_slice,:]\n",
        "  val_data = train_val_data[val_slice,:]\n",
        "\n",
        "  # load test data\n",
        "  test_data = np.concatenate([np.loadtxt(path, unpack=True) for path in test_paths])\n",
        "\n",
        "  # save train, val, test data to file\n",
        "  np.savetxt(datafolder + train_file, train_data)\n",
        "  np.savetxt(datafolder + val_file, val_data)\n",
        "  np.savetxt(datafolder + test_file, test_data)\n",
        "\n",
        "\n",
        "# load train, val, test data\n",
        "train_data = np.loadtxt(datafolder + train_file, unpack=False)\n",
        "val_data = np.loadtxt(datafolder + val_file, unpack=False)\n",
        "test_data = np.loadtxt(datafolder + test_file, unpack=False)"
      ],
      "metadata": {
        "id": "COGqq8uQXVCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape, val_data.shape, test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qe0IZzgadbP",
        "outputId": "cd76ad21-7fbe-465c-ae96-0d1289aeb4e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((203800, 149), (50950, 149), (139587, 149))"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_data = np.concatenate((train_data, val_data), axis=0) # Consider both train and val data as one\n",
        "train_data_copy = train_val_data.copy()\n",
        "\n",
        "train_input_data = train_data_copy[:,:40] # take the features\n",
        "train_labels = train_data_copy[:,-5:] # take the labels\n",
        "\n",
        "train_input_data.shape, train_labels.shape"
      ],
      "metadata": {
        "id": "8LYKODMvJqB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🪡🧵 Adapting the Method "
      ],
      "metadata": {
        "id": "Y_wowU-2IV83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. compute the *mid-prices* over all the timestamps"
      ],
      "metadata": {
        "id": "1QfZcrOtBD3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_mid_prices(lob_data):\n",
        "  lob_data_copy = lob_data.copy()\n",
        "  mid_prices = (lob_data_copy[:,0] + lob_data_copy[:,2])/2\n",
        "  return mid_prices "
      ],
      "metadata": {
        "id": "XoZ1AzlQBPKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mid_prices = compute_mid_prices(train_input_data)\n",
        "mid_prices.shape\n",
        "# compute the mean over all the mid-prices (probably is over the windows instead, but for now do it this way)\n",
        "#m_d = mid_prices.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O14ZWzfbB2If",
        "outputId": "4c8e70c8-b9aa-45f0-951f-a99c050f3b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(254750,)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. compute the *mid-prices* means within each time window"
      ],
      "metadata": {
        "id": "LeqgKrqh81DJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Future mean ($F_d$)"
      ],
      "metadata": {
        "id": "FCJLiXX7E33d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def future_avg_mid_prices(mid_prices, window_shape=3):\n",
        "  \"\"\"computes the avg mid-prices of a stock in the next K timeslots\"\"\"\n",
        "\n",
        "\n",
        "  K = window_shape\n",
        "  F_d = np.lib.stride_tricks.sliding_window_view(mid_prices, window_shape=K) # F_d.shape = (203797, 3), i.e.: n - (K-1)\n",
        "  F_d = F_d[1:,:] # F_d.shape = (203797, 3) # for each timestamp, we consider the slide_view starting from the very next slot\n",
        "  F_d = F_d.mean(axis=1) # here I normalize, while the paper does not\n",
        "  F_d = F_d[K-1:] # we cutoff the first K-1 elements since for those we cannot compute the average mid-price of past K days -> we must have matching pairs \n",
        "  \n",
        "  return F_d"
      ],
      "metadata": {
        "id": "UqEx12PjC6Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Past mean ($B_d$)"
      ],
      "metadata": {
        "id": "IXgRP9C2L4J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def past_avg_mid_prices(mid_prices, window_shape=3):\n",
        "  \"\"\"computes the avg mid-prices of a stock per each slot in the past K timeslots, including that slot\"\"\"\n",
        "\n",
        "  K = window_shape\n",
        "  B_d = np.lib.stride_tricks.sliding_window_view(mid_prices[::-1], window_shape=K)\n",
        "  B_d = B_d.mean(axis=1) # here I normalize, while the paper does not\n",
        "  B_d = B_d[K:]\n",
        "  B_d = B_d[::-1]\n",
        "\n",
        "  return B_d"
      ],
      "metadata": {
        "id": "v8b66LTBL2_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall mean ($μ_d$)"
      ],
      "metadata": {
        "id": "k2tMWeDdJ-pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_means(mid_prices, window_shape=3):\n",
        "\n",
        "  future_avg_mid_prices_vals = future_avg_mid_prices(mid_prices, window_shape=window_shape)\n",
        "  past_avg_mid_prices_vals = past_avg_mid_prices(mid_prices, window_shape=window_shape)\n",
        "  return (future_avg_mid_prices_vals + past_avg_mid_prices_vals) / 2"
      ],
      "metadata": {
        "id": "uAiBRrD-OvnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***nb***: if we have $n$ timestamps, we end up having the mid-prices averages only for: $n - (K-1) - (K)$ of those"
      ],
      "metadata": {
        "id": "CvxdOYj-oxid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. compute the slope"
      ],
      "metadata": {
        "id": "2p60QBTdK5Dw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slope $\\delta_d$"
      ],
      "metadata": {
        "id": "YY1mLs4rgiIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def slope_d(future_avg_mid_prices, past_avg_mid_prices):\n",
        "  return future_avg_mid_prices - past_avg_mid_prices"
      ],
      "metadata": {
        "id": "yZugiP2ughNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. compute the standard deviation for stock trend over 2K days"
      ],
      "metadata": {
        "id": "QHfArYmALBT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard deviation ($\\sigma_d$)"
      ],
      "metadata": {
        "id": "5XM01xaZibE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stock_std_dev(mid_prices, window_shape=3):\n",
        "  \"\"\"computes the standard deviation of a stock for each timeslot, during the 2K past/future timeslots\"\"\"\n",
        "  \n",
        "  K = window_shape\n",
        "\n",
        "  means = np.expand_dims(compute_means(mid_prices, window_shape=K), axis=1)\n",
        "\n",
        "  Fs_d = np.lib.stride_tricks.sliding_window_view(mid_prices, window_shape=K) # F_d.shape = (203797, 3), i.e.: n - (K-1)\n",
        "  Fs_d = Fs_d[1:,:]\n",
        "  Fs_d = Fs_d[K-1:]\n",
        "  #Fs_d = np.power(Fs_d - means, 2)\n",
        "  Fs_d = np.sum(Fs_d - means, axis=1)\n",
        "\n",
        "  Bs_d = np.lib.stride_tricks.sliding_window_view(mid_prices[::-1], window_shape=K)\n",
        "  Bs_d = Bs_d[K:]\n",
        "  Bs_d = Bs_d[::-1]\n",
        "  #Bs_d = np.power(Bs_d - means, 2)\n",
        "  Bs_d = np.sum(Bs_d - means, axis=1)\n",
        "\n",
        "  Tot_d = np.power(Fs_d + Bs_d,2)\n",
        "  s_d = np.sqrt(Tot_d/(2*K))\n",
        "\n",
        "  #s_d = np.sqrt(np.sum(Fs_d + Bs_d, axis=1)/(2*K))\n",
        "  \n",
        "  return s_d"
      ],
      "metadata": {
        "id": "qhHRV0JspZK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Slope detection labelling method implementation"
      ],
      "metadata": {
        "id": "dPUSA1B6s2f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def slope_detection_labelling(mid_prices, window_shape=3, peak=True):\n",
        "  \"\"\"returns the labels according to the slope_detection labelling method as\n",
        "  described in Short Term Stock Price-Trend Prediction paper\n",
        "  \n",
        "  Since in the \"Forecasting Stock Prices from the Limit Order Book\" paper we had\n",
        "  only 3 labels (UP, STATIONARY, DOWN), to make the new ones comparable, I will\n",
        "  ignore the \"rise\" and \"fall\" labels, since their formulation is close to the\n",
        "  ones propoesed in the aforementioned paper. Instead, I will compute the \"+\"\n",
        "  labels, and defaulting the other values to be considered as \"stationary\"\n",
        "\n",
        "  label mapping:\n",
        "    1 -> rise+\n",
        "    2 -> stationary (default)\n",
        "    3 -> fall+\n",
        "  \"\"\"\n",
        "\n",
        "  K = window_shape\n",
        "  means = compute_means(mid_prices, window_shape=K) # mid_prices.mean()\n",
        "  slope = slope_d(future_avg_mid_prices(mid_prices, window_shape=K), past_avg_mid_prices(mid_prices, window_shape=K))\n",
        "  std_dev = stock_std_dev(mid_prices, window_shape=K)\n",
        "\n",
        "  mid_prices_copy = mid_prices.copy()\n",
        "  mid_prices_copy = mid_prices_copy[K-1:-K]\n",
        "\n",
        "  if peak:\n",
        "    labels = np.zeros_like(mid_prices_copy, dtype=int) + 2\n",
        "    rise_p_idx = np.where(np.greater(np.array(mid_prices_copy),(np.array(means) + np.array(std_dev))) & (np.greater(np.array(slope),0)))\n",
        "    #print(\"rise_p_idx: {}, len: {})\".format(rise_p_idx[0], rise_p_idx[0].size))\n",
        "    fall_p_idx = np.where(np.less(np.array(mid_prices_copy),(np.array(means) - np.array(std_dev))) & (np.less(np.array(slope),0)))\n",
        "    \n",
        "    # set the labels\n",
        "    labels[rise_p_idx] = 3 # the price will fall \n",
        "    labels[fall_p_idx] = 1 # the price will rise\n",
        "\n",
        "  else:\n",
        "    pass\n",
        "    #TODO: implement the logic for including the standard rise and fall\n",
        "  \n",
        "  return labels"
      ],
      "metadata": {
        "id": "OG76a4Y6s0H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🏃 Create the labels!"
      ],
      "metadata": {
        "id": "NMlATyabLXzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ks = [1,2,3,5,10]\n",
        "maxK = np.max(Ks)\n",
        "\n",
        "new_train_labels = np.zeros_like(train_labels)\n",
        "\n",
        "for i, K in enumerate(Ks):\n",
        "  new_labels = slope_detection_labelling(mid_prices, window_shape=K)\n",
        "  #sns.histplot(new_labels)\n",
        "  #plt.show()\n",
        "  \n",
        "  #print(i, \")\", new_labels.shape[0])\n",
        "  new_train_labels[K-1:-K,i] = new_labels"
      ],
      "metadata": {
        "id": "dDw1L70x2Rin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_train_labels[:maxK,:], \"\\n\\n\" , new_train_labels[-maxK:,:]) # these should be eliminated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soIF3gU2MaaD",
        "outputId": "fcb6c2cc-10ad-4e65-9e98-f3fbc1095e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2. 0. 0. 0. 0.]\n",
            " [2. 2. 0. 0. 0.]\n",
            " [2. 1. 1. 0. 0.]\n",
            " [2. 2. 2. 0. 0.]\n",
            " [2. 2. 2. 2. 0.]\n",
            " [2. 2. 2. 2. 0.]\n",
            " [2. 2. 2. 2. 0.]\n",
            " [2. 3. 3. 3. 0.]\n",
            " [2. 2. 3. 3. 0.]\n",
            " [2. 3. 3. 3. 3.]\n",
            " [2. 2. 3. 3. 3.]] \n",
            "\n",
            " [[2. 2. 2. 2. 0.]\n",
            " [2. 2. 2. 2. 0.]\n",
            " [2. 3. 3. 3. 0.]\n",
            " [2. 2. 3. 3. 0.]\n",
            " [2. 2. 2. 3. 0.]\n",
            " [2. 2. 2. 0. 0.]\n",
            " [2. 2. 2. 0. 0.]\n",
            " [2. 2. 0. 0. 0.]\n",
            " [2. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trim the labels\n",
        "\n",
        "To have valid labels for all the timestamps in a single data structure, remove from all the horizons (*i.e.,* columns) the first (maxK-1) and the last (maxK) rows"
      ],
      "metadata": {
        "id": "DGosDewjMdGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_labels = new_train_labels[maxK:-maxK,:]"
      ],
      "metadata": {
        "id": "rG7PqEnzM_fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check for labels correspondance\n",
        "\n",
        "Verify if there's some correspondance between the old and new labels.\n",
        "\n",
        "The number in the `()` is the encoding of the label.\n",
        "\n",
        "**Old labels**: $$l(p_d) = \\begin{cases}\n",
        "    rise \\,(1),& \\text{if } F_d \\gt B_d \\cdot (1 + \\alpha)\\\\\n",
        "    fall \\,(3),& \\text{if } F_d \\lt B_d \\cdot (1 - \\alpha)\\\\\n",
        "    stationary \\,(2),& otherwise\\\\ \n",
        "\\end{cases}$$\n",
        "\n",
        "New labels: $$l(p_d) = \\begin{cases}\n",
        "    rise+,& \\text{if } p_d \\gt (\\mu_d + \\sigma_d) \\,\\text{ and }\\ \\delta_d \\gt 0\\\\\n",
        "    rise,& \\text{if } \\text{only } \\delta_d \\gt 0  \\text{ holds}\\\\\n",
        "    fall+,& \\text{if } p_d \\lt (\\mu_d - \\sigma_d) \\,\\text{ and }\\ \\delta_d \\lt 0\\\\ \n",
        "    fall,& \\text{if } \\text{only } \\delta_d \\lt 0  \\text{ holds}\\\\\n",
        "\\end{cases}$$\n",
        "\n",
        "\\\n",
        "\n",
        "Since it may be of our interest to know whether the trend will change (either positively or negatively), keep the \"$+$\" labels: they denote an inversion. The other two labels are \"merged\" into a single one, *e.g.*:  \"stationary\". This one denotes the fact that the trend will remain unaltered.\n",
        "\n",
        "To maintain a the coherence between the ordering and the meaningfullness of the labels, the new ones will be encoded as follow:\n",
        "\n",
        "**New labels**: $$l(p_d) = \\begin{cases}\n",
        "    fall+ \\,(1),& \\text{if } p_d \\lt (\\mu_d - \\sigma_d) \\,\\text{ and }\\ \\delta_d \\lt 0\\\\\n",
        "    rise+ \\,(3),& \\text{if } p_d \\gt (\\mu_d + \\sigma_d) \\,\\text{ and }\\ \\delta_d \\gt 0\\\\\n",
        "    unaltered \\,(2),& otherwise\\\\    \n",
        "\\end{cases}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "s-R0uV8Yd8-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_labels_correspondance(old_labels, new_labels):\n",
        "  \"\"\"count the number of correspondances between the old and new labels considering the same position\"\"\"\n",
        "\n",
        "  assert old_labels.size == new_labels.size, \"The number of labels in the vectors must coincide\"\n",
        "\n",
        "  return np.round((np.sum(old_labels == new_labels) * 100 )/ old_labels.size, 2)\n",
        "\n",
        "# check the correspondance (in %) between the old and new labels, for all the horizons\n",
        "\n",
        "for i in range(5):\n",
        "  #print(np.unique(train_labels[:,-i]), np.unique(new_train_labels[:,-i]) )\n",
        "  print(Ks[i], count_labels_correspondance(train_labels[maxK:-maxK,-i], new_train_labels[:,-i]), \"%\")"
      ],
      "metadata": {
        "id": "wxS3ACckOR_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "186cea76-2361-41d4-9a34-b56ec65727a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 60.53 %\n",
            "2 23.26 %\n",
            "3 33.67 %\n",
            "5 42.49 %\n",
            "10 49.4 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save new Train, Val labels\n",
        "Prepare the train data (input features + new labels) to be saved"
      ],
      "metadata": {
        "id": "z3mfkNEKRkSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_data = np.concatenate((train_data_copy[maxK:-maxK,:-5], new_train_labels), axis=1)\n",
        "new_train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh5OA4ufNF7h",
        "outputId": "d1bbd150-5935-4fe0-93b0-63e3b2dfb60d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(254730, 149)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split into train and validation\n",
        "train_slice = slice(0, int(0.8 * new_train_data.shape[0]))\n",
        "val_slice = slice(int(0.8 * new_train_data.shape[0]), new_train_data.shape[0])\n",
        "\n",
        "\n",
        "# maintain this order, otherwise new_val_data will be empty\n",
        "new_val_data = new_train_data[val_slice,:]\n",
        "new_train_data = new_train_data[train_slice,:]\n",
        "\n",
        "\n",
        "new_train_data.shape, new_val_data.shape"
      ],
      "metadata": {
        "id": "f3JEc9Z0LGve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daa6f0d1-8313-49a4-d0e0-ccc28ef5e0bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((203784, 149), (50946, 149))"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save train, val, test data to file\n",
        "np.savetxt(datafolder + new_train_file, new_train_data)\n",
        "np.savetxt(datafolder + new_val_file, new_val_data)"
      ],
      "metadata": {
        "id": "DCpTAtMn6nJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Test labels\n",
        "\n",
        "Now perform the same operation on *test* data, then save these data on Drive"
      ],
      "metadata": {
        "id": "H2GdVzSS0I47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_copy = test_data.copy()\n",
        "test_input_data = test_data_copy[:,:40] # take the features\n",
        "test_labels = test_data_copy[:,-5:] # take the labels\n",
        "test_input_data.shape, test_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw2lu4UrVuRt",
        "outputId": "059b1afd-7633-4d30-d90d-33b51b24b01b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((139587, 40), (139587, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_mid_prices = compute_mid_prices(test_input_data)\n",
        "test_mid_prices.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6Sm5ibnWLte",
        "outputId": "ad9ab43b-74cc-400f-b5fb-f669ced86ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(139587,)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ks = [1,2,3,5,10]\n",
        "maxK = np.max(Ks)\n",
        "\n",
        "new_test_labels = np.zeros_like(test_labels)\n",
        "\n",
        "for i, K in enumerate(Ks):\n",
        "  new_labels = slope_detection_labelling(test_mid_prices, window_shape=K)\n",
        "  #sns.histplot(new_labels)\n",
        "  #plt.show()\n",
        "  \n",
        "  #print(i, \")\", new_labels.shape[0])\n",
        "  new_test_labels[K-1:-K,i] = new_labels"
      ],
      "metadata": {
        "id": "K1zrfKTZWV3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_test_labels = new_test_labels[maxK:-maxK,:]\n",
        "\n",
        "for i in range(5): # 5 is hardcoded -> it is the # horizons\n",
        "  #print(np.unique(train_labels[:,-i]), np.unique(new_train_labels[:,-i]) )\n",
        "  print(Ks[i], count_labels_correspondance(test_labels[maxK:-maxK,-i], new_test_labels[:,-i]), \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDWbbHvvXC0A",
        "outputId": "a47ee901-5453-4e23-9b96-f439dca1adcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 70.67 %\n",
            "2 37.88 %\n",
            "3 49.01 %\n",
            "5 56.64 %\n",
            "10 62.04 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_test_data = np.concatenate((test_data_copy[maxK:-maxK,:-5], new_test_labels), axis=1)\n",
        "new_test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3Hp8z0TXm7x",
        "outputId": "8c8bca8c-c7b5-44c6-e4b0-9a34054231a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(139567, 149)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save new test data to file\n",
        "np.savetxt(datafolder + new_test_file, new_test_data)"
      ],
      "metadata": {
        "id": "u9awGUeTXuzO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}